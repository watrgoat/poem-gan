{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\olive\\poem-gan\\venv\\lib\\site-packages (from -r requirements.txt (line 1)) (1.5.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\olive\\poem-gan\\venv\\lib\\site-packages (from -r requirements.txt (line 2)) (4.11.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\olive\\poem-gan\\venv\\lib\\site-packages (from -r requirements.txt (line 3)) (1.24.2)\n",
      "Requirement already satisfied: langdetect in c:\\users\\olive\\poem-gan\\venv\\lib\\site-packages (from -r requirements.txt (line 4)) (1.0.9)\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\olive\\poem-gan\\venv\\lib\\site-packages (from -r requirements.txt (line 6)) (3.8.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\olive\\poem-gan\\venv\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\olive\\poem-gan\\venv\\lib\\site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\olive\\poem-gan\\venv\\lib\\site-packages (from beautifulsoup4->-r requirements.txt (line 2)) (2.4)\n",
      "Requirement already satisfied: six in c:\\users\\olive\\poem-gan\\venv\\lib\\site-packages (from langdetect->-r requirements.txt (line 4)) (1.16.0)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\olive\\poem-gan\\venv\\lib\\site-packages (from nltk->-r requirements.txt (line 6)) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\olive\\poem-gan\\venv\\lib\\site-packages (from nltk->-r requirements.txt (line 6)) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\olive\\poem-gan\\venv\\lib\\site-packages (from nltk->-r requirements.txt (line 6)) (2022.10.31)\n",
      "Requirement already satisfied: click in c:\\users\\olive\\poem-gan\\venv\\lib\\site-packages (from nltk->-r requirements.txt (line 6)) (8.1.3)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "     ---------------------------------------- 0.0/289.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 289.9/289.9 kB 8.7 MB/s eta 0:00:00\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.0.0-cp39-cp39-win_amd64.whl (39 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\olive\\poem-gan\\venv\\lib\\site-packages (from click->nltk->-r requirements.txt (line 6)) (0.4.6)\n",
      "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# poem generation\n",
    "# Deep learning: a combination of VAE/GAN\n",
    "# condition the generator of the GAN for rubustness\n",
    "# latent space!!\n",
    "# how do you evaluate these?\n",
    "# discriminator\n",
    "\n",
    "# test using a model on text and poems to predict whether a given text is a poem or not.\n",
    "# maybe no input!\n",
    "# my goal will be to generate 10,000 poems\n",
    "\n",
    "\n",
    "# future project would be to write books using ai. minimum of 10-50 pages based on the most popular topic on xyz site?\n",
    "# based on stars guess where the picture was taken in space?"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 1,
=======
   "execution_count": 2,
>>>>>>> e5f06c3a75af07c2266b6a8c27b48b931ba9cd56
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import string\n",
    "import logging\n",
    "import re\n",
    "import contractions\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 7,
>>>>>>> e5f06c3a75af07c2266b6a8c27b48b931ba9cd56
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26122\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_pickle('ready_poems.pickle')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(df)):\n",
    "    if 'Click the icon above' in df.content[i] or 'Click on the icon above' in df.content[i]:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HPP Notes\n",
    "# 2018: \n",
    "# Owned 17.7m sqft\n",
    "# office space: 13.9m sqft\n",
    "# office weighted avg lease: 5.1 years\n",
    "# 52 diff properties\n",
    "\n",
    "# Tenant rent sheet has changed\n",
    "# top 3: google, netflix, square\n",
    "# mostly tech company\n",
    "\n",
    "# 2022\n",
    "# \n",
    "\n",
    "# tenant diverse:\n",
    "# top 3: google, amazon, netflix\n",
    "# google is paying double, and the rest have stayed the same, only minor increases\n",
    "# 40% based on tech spaces. need tech companies to stay in-person\n",
    "# they are looking heavier on lease expiration (more leases will expire sooner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35130071927440.19\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gingerit to fix grammar after? idk\n",
    "# seperate model for punctuation and capititalizaiton. train on the lowercase data and the punctuated, capitalized, etc data to reintroduce some form of grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.content = cleaner(df.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
=======
   "execution_count": 11,
>>>>>>> e5f06c3a75af07c2266b6a8c27b48b931ba9cd56
   "metadata": {},
   "outputs": [],
   "source": [
    "poems = df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems = poems.map(lambda x: nltk.tokenize.word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set([word for poem in poems for word in poem])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(texts):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = ids_from_chars(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[19148, 34574, 104864, 74429, 46874, 96371, 126471],\n",
       " [74695, 74625, 127913]]>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = chars_from_ids(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
<<<<<<< HEAD
    "df = pd.read_pickle('ready_poems.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "poems = df.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26582"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(poems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_pickle('mostly_english_poems.pickle')\n",
    "df1.to_csv('mostly_english_poems.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ready_poems.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Poems = df.content.map(lambda x: nltk.tokenize.word_tokenize(x))"
=======
    "# remove translated from/by"
>>>>>>> e5f06c3a75af07c2266b6a8c27b48b931ba9cd56
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace \\n with newline\n",
    "# expand contractions\n",
    "# split into tokens\n",
    "# convert into lowecase\n",
    "# Decoding Unicode characters into a normalized form, such as UTF8???\n",
    "# remove numbers???\n",
    "# remove all punctuation\n",
    "# filter out non alphabetic\n",
    "# recontruct into poems\n",
    "# us isalpha\n",
    "# string = re.sub(r\"[,.”;@—#“…?!&$-’_)–(*)]+\", ' ', string)\n",
    "# string = re.sub('\\s\\s+' , ' ', cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log the devices being used for tensorflow operations\n",
    "tf.debugging.set_log_device_placement(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a940dd860dc355cf12de7c6e789bde0b494d41255d0443cce9e7aeeb3f61f9d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
